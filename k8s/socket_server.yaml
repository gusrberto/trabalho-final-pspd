apiVersion: apps/v1
kind: Deployment
metadata:
  name: socket-server
  namespace: default
  labels:
    app: socket-server
spec:
  replicas: 2
  selector:
    matchLabels:
      app: socket-server
  template:
    metadata:
      labels:
        app: socket-server
    spec:
      serviceAccountName: socket-server
      containers:
      - name: socket-server
        image: socket-server:latest
        imagePullPolicy: Never
        ports:
        - containerPort: 5000
        workingDir: /app
        volumeMounts:
        - name: socket-server-files
          mountPath: /app
        resources:
          requests:
            cpu: "100m"
            memory: "128Mi"
          limits:
            cpu: "500m"
            memory: "256Mi"
      volumes:
      - name: socket-server-files
        configMap:
          name: socket-server-config
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: socket-server-config
data:
  socket_server.py: |
    # server.py

    import socket
    import subprocess
    import threading
    import time
    import sys
    import os
    from kubernetes import client, config
    import yaml

    # Adicionar o diretório pai ao path para importar o coletor de métricas
    sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    from metrics_collector import metrics_collector

    HOST = '0.0.0.0'
    PORT = 5000

    # Carregar configuração dentro do cluster
    config.load_incluster_config()
    api = client.CustomObjectsApi()

    def create_spark_app(powmin, powmax, job_id):
        # Simular criação de job por enquanto
        print(f"Simulando criação de job Spark: {powmin}, {powmax}")
        return f"game-of-life-{job_id}"

    def wait_for_completion(name):
        # Simular espera por conclusão
        print(f"Simulando espera por conclusão do job: {name}")
        time.sleep(3)  # Simular processamento
        return "COMPLETED"

    def filter_game_result(logs: str) -> str:
        lines = logs.splitlines()
        filtered = []
        for line in lines:
            line = line.strip()
            # verificar se linha tem o formato x,y,1 (3 valores, último é 1, x e y inteiros)
            parts = line.split(",")
            if len(parts) == 3 and parts[2] == "1":
                try:
                    x = int(parts[0])
                    y = int(parts[1])
                    filtered.append(line)
                except ValueError:
                    pass
        if not filtered:
            return "[nenhuma célula viva]"
        return "\n".join(filtered)

    def get_driver_logs(app_name, namespace="spark"):
        # Simular logs do driver
        print(f"Simulando logs do driver para: {app_name}")
        return "1,2,1\n2,3,1\n3,1,1\n3,2,1\n3,3,1"

    def handle_client(conn, addr):
        client_id = f"{addr[0]}:{addr[1]}"
        print(f"[+] Conexão de {client_id}")
        
        # Registrar conexão do cliente
        try:
            metrics_collector.record_client_connection(client_id)
        except Exception as e:
            print(f"[!] Erro ao registrar conexão: {e}")
        
        with conn:
            try:
                data = conn.recv(1024)
                if not data:
                    conn.sendall(b"Erro: Nenhum dado recebido.\n")
                    return

                try:
                    powmin, powmax = map(int, data.decode().strip().split(","))
                except ValueError:
                    conn.sendall(b"Erro: Formato invalido. Use: inteiro,inteiro\n")
                    return

                job_id = int(time.time())
                start_time = time.time()
                
                # Registrar início do job
                try:
                    metrics_collector.record_job_start(
                        client_id=client_id,
                        engine_type="spark",
                        grid_size=powmax - powmin,
                        iterations=powmax - powmin
                    )
                except Exception as e:
                    print(f"[!] Erro ao registrar início do job: {e}")
                
                app_name = create_spark_app(powmin, powmax, job_id)

                conn.sendall(f"JOB {app_name} criado. Aguardando conclusão...\n".encode())

                state = wait_for_completion(app_name)
                execution_time_ms = int((time.time() - start_time) * 1000)

                if state == "COMPLETED":
                    logs = get_driver_logs(app_name)
                    filtered_logs = filter_game_result(logs)
                    output_size = len(filtered_logs.split('\n')) if filtered_logs != "[nenhuma célula viva]" else 0
                    
                    # Registrar conclusão do job
                    try:
                        metrics_collector.record_job_completion(
                            client_id=client_id,
                            engine_type="spark",
                            execution_time_ms=execution_time_ms,
                            status="completed",
                            grid_size=powmax - powmin,
                            iterations=powmax - powmin
                        )
                    except Exception as e:
                        print(f"[!] Erro ao registrar conclusão do job: {e}")
                    
                    response = f"SUCESSO JOB {app_name} finalizado com sucesso!\nResultado:\n{filtered_logs}\n"
                else:
                    # Registrar falha do job
                    try:
                        metrics_collector.record_job_completion(
                            client_id=client_id,
                            engine_type="spark",
                            execution_time_ms=execution_time_ms,
                            status="failed",
                            grid_size=powmax - powmin,
                            iterations=powmax - powmin
                        )
                    except Exception as e:
                        print(f"[!] Erro ao registrar falha do job: {e}")
                    
                    response = f"FALHA JOB {app_name} falhou na execução.\n"

                conn.sendall(response.encode())

            except Exception as e:
                error_msg = f"Erro interno do servidor: {str(e)}\n"
                conn.sendall(error_msg.encode())
                print(f"[!] Erro com {client_id}: {e}")
            finally:
                # Registrar desconexão do cliente
                try:
                    metrics_collector.record_client_disconnection(client_id)
                except Exception as e:
                    print(f"[!] Erro ao registrar desconexão: {e}")
                
                print(f"[-] Desconectado {client_id}")

    def main():
        srv = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        srv.bind((HOST, PORT))
        srv.listen()
        print(f"[+] Servidor escutando {HOST}:{PORT}")

        while True:
            conn, addr = srv.accept()
            t = threading.Thread(target=handle_client, args=(conn, addr), daemon=True)
            t.start()

    if __name__ == "__main__":
        main()

  metrics_collector.py: |
    import json
    import time
    import requests
    from datetime import datetime, timedelta
    from typing import Dict, List, Optional
    import threading
    import logging

    # Configurar logging
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)

    class MetricsCollector:
        def __init__(self, elasticsearch_url: str = "http://elasticsearch.elasticsearch.svc.cluster.local:9200"):
            self.elasticsearch_url = elasticsearch_url
            self.index_name = "game_of_life_metrics"
            self.session = requests.Session()
            self.session.headers.update({'Content-Type': 'application/json'})
            
            # Inicializar índice se não existir
            self._create_index_if_not_exists()
        
        def _create_index_if_not_exists(self):
            """Cria o índice no ElasticSearch se não existir"""
            try:
                # Verificar se o índice existe
                response = self.session.head(f"{self.elasticsearch_url}/{self.index_name}")
                if response.status_code == 404:
                    # Criar índice com mapping
                    mapping = {
                        "mappings": {
                            "properties": {
                                "timestamp": {"type": "date"},
                                "client_id": {"type": "keyword"},
                                "engine_type": {"type": "keyword"},
                                "execution_time_ms": {"type": "long"},
                                "status": {"type": "keyword"},
                                "grid_size": {"type": "integer"},
                                "iterations": {"type": "integer"},
                                "total_requests": {"type": "long"},
                                "concurrent_clients": {"type": "integer"},
                                "request_type": {"type": "keyword"}
                            }
                        }
                    }
                    
                    response = self.session.put(
                        f"{self.elasticsearch_url}/{self.index_name}",
                        json=mapping
                    )
                    
                    if response.status_code == 200:
                        logger.info(f"Índice {self.index_name} criado com sucesso")
                    else:
                        logger.error(f"Erro ao criar índice: {response.text}")
                else:
                    logger.info(f"Índice {self.index_name} já existe")
                    
            except Exception as e:
                logger.error(f"Erro ao verificar/criar índice: {e}")
        
        def record_job_start(self, client_id: str, engine_type: str, grid_size: int, iterations: int):
            """Registra o início de um job"""
            try:
                doc = {
                    "timestamp": datetime.utcnow().isoformat(),
                    "client_id": client_id,
                    "engine_type": engine_type,
                    "status": "started",
                    "grid_size": grid_size,
                    "iterations": iterations,
                    "request_type": "job_start"
                }
                
                response = self.session.post(
                    f"{self.elasticsearch_url}/{self.index_name}/_doc",
                    json=doc
                )
                
                if response.status_code == 201:
                    logger.info(f"Job iniciado registrado: {client_id} - {engine_type}")
                else:
                    logger.error(f"Erro ao registrar início do job: {response.text}")
                    
            except Exception as e:
                logger.error(f"Erro ao registrar início do job: {e}")
        
        def record_job_completion(self, client_id: str, engine_type: str, execution_time_ms: int, 
                                status: str = "completed", grid_size: int = None, iterations: int = None):
            """Registra a conclusão de um job"""
            try:
                doc = {
                    "timestamp": datetime.utcnow().isoformat(),
                    "client_id": client_id,
                    "engine_type": engine_type,
                    "execution_time_ms": execution_time_ms,
                    "status": status,
                    "request_type": "job_completion"
                }
                
                if grid_size:
                    doc["grid_size"] = grid_size
                if iterations:
                    doc["iterations"] = iterations
                
                response = self.session.post(
                    f"{self.elasticsearch_url}/{self.index_name}/_doc",
                    json=doc
                )
                
                if response.status_code == 201:
                    logger.info(f"Job completado registrado: {client_id} - {engine_type} - {execution_time_ms}ms")
                else:
                    logger.error(f"Erro ao registrar conclusão do job: {response.text}")
                    
            except Exception as e:
                logger.error(f"Erro ao registrar conclusão do job: {e}")
        
        def record_client_connection(self, client_id: str):
            """Registra conexão de um cliente"""
            try:
                doc = {
                    "timestamp": datetime.utcnow().isoformat(),
                    "client_id": client_id,
                    "request_type": "client_connection"
                }
                
                response = self.session.post(
                    f"{self.elasticsearch_url}/{self.index_name}/_doc",
                    json=doc
                )
                
                if response.status_code == 201:
                    logger.info(f"Conexão de cliente registrada: {client_id}")
                else:
                    logger.error(f"Erro ao registrar conexão: {response.text}")
                    
            except Exception as e:
                logger.error(f"Erro ao registrar conexão: {e}")
        
        def record_client_disconnection(self, client_id: str):
            """Registra desconexão de um cliente"""
            try:
                doc = {
                    "timestamp": datetime.utcnow().isoformat(),
                    "client_id": client_id,
                    "request_type": "client_disconnection"
                }
                
                response = self.session.post(
                    f"{self.elasticsearch_url}/{self.index_name}/_doc",
                    json=doc
                )
                
                if response.status_code == 201:
                    logger.info(f"Desconexão de cliente registrada: {client_id}")
                else:
                    logger.error(f"Erro ao registrar desconexão: {response.text}")
                    
            except Exception as e:
                logger.error(f"Erro ao registrar desconexão: {e}")
        
        def get_metrics_summary(self, time_range_hours: int = 24) -> Dict:
            """Obtém resumo das métricas das últimas N horas"""
            try:
                # Calcular timestamp de início
                start_time = datetime.utcnow() - timedelta(hours=time_range_hours)
                
                # Query para métricas gerais
                query = {
                    "query": {
                        "bool": {
                            "must": [
                                {"range": {"timestamp": {"gte": start_time.isoformat()}}},
                                {"terms": {"request_type": ["job_completion", "client_connection"]}}
                            ]
                        }
                    },
                    "aggs": {
                        "total_jobs": {
                            "filter": {"term": {"request_type": "job_completion"}},
                            "aggs": {
                                "avg_execution_time": {"avg": {"field": "execution_time_ms"}},
                                "success_rate": {
                                    "filter": {"term": {"status": "completed"}}
                                }
                            }
                        },
                        "engine_distribution": {
                            "filter": {"term": {"request_type": "job_completion"}},
                            "aggs": {
                                "engines": {"terms": {"field": "engine_type"}}
                            }
                        },
                        "execution_times": {
                            "filter": {"term": {"request_type": "job_completion"}},
                            "aggs": {
                                "times": {
                                    "date_histogram": {
                                        "field": "timestamp",
                                        "calendar_interval": "1h"
                                    },
                                    "aggs": {
                                        "avg_time": {"avg": {"field": "execution_time_ms"}}
                                    }
                                }
                            }
                        },
                        "concurrent_clients": {
                            "filter": {"term": {"request_type": "client_connection"}},
                            "aggs": {
                                "clients": {
                                    "date_histogram": {
                                        "field": "timestamp",
                                        "calendar_interval": "1h"
                                    },
                                    "aggs": {
                                        "unique_clients": {"cardinality": {"field": "client_id"}}
                                    }
                                }
                            }
                        }
                    }
                }
                
                response = self.session.post(
                    f"{self.elasticsearch_url}/{self.index_name}/_search",
                    json=query
                )
                
                if response.status_code == 200:
                    result = response.json()
                    return self._process_metrics_result(result)
                else:
                    logger.error(f"Erro ao buscar métricas: {response.text}")
                    return self._get_default_metrics()
                    
            except Exception as e:
                logger.error(f"Erro ao obter métricas: {e}")
                return self._get_default_metrics()
        
        def _process_metrics_result(self, result: Dict) -> Dict:
            """Processa o resultado da query do ElasticSearch"""
            try:
                aggs = result.get("aggregations", {})
                
                # Total de jobs
                total_jobs_bucket = aggs.get("total_jobs", {})
                total_jobs = total_jobs_bucket.get("doc_count", 0)
                
                # Tempo médio de execução
                avg_execution_time = total_jobs_bucket.get("avg_execution_time", {}).get("value", 0)
                
                # Taxa de sucesso
                success_bucket = total_jobs_bucket.get("success_rate", {})
                success_count = success_bucket.get("doc_count", 0)
                success_rate = success_count / total_jobs if total_jobs > 0 else 0
                
                # Distribuição por engine
                engine_dist = aggs.get("engine_distribution", {}).get("engines", {}).get("buckets", [])
                engines = [bucket["key"] for bucket in engine_dist]
                engine_counts = [bucket["doc_count"] for bucket in engine_dist]
                
                # Tempos de execução ao longo do tempo
                execution_times_bucket = aggs.get("execution_times", {}).get("times", {}).get("buckets", [])
                time_labels = []
                execution_times = []
                
                for bucket in execution_times_bucket:
                    time_labels.append(bucket["key_as_string"][:16])  # Formato: YYYY-MM-DDTHH:MM
                    execution_times.append(bucket["avg_time"]["value"] or 0)
                
                # Clientes simultâneos
                concurrent_clients_bucket = aggs.get("concurrent_clients", {}).get("clients", {}).get("buckets", [])
                concurrent_clients = []
                for bucket in concurrent_clients_bucket:
                    concurrent_clients.append(bucket["unique_clients"]["value"] or 0)
                
                return {
                    "total_jobs": total_jobs,
                    "avg_execution_time": round(avg_execution_time, 2),
                    "success_rate": round(success_rate, 2),
                    "engines": engines,
                    "engine_counts": engine_counts,
                    "execution_times": execution_times,
                    "time_labels": time_labels,
                    "concurrent_clients": concurrent_clients
                }
                
            except Exception as e:
                logger.error(f"Erro ao processar métricas: {e}")
                return self._get_default_metrics()
        
        def _get_default_metrics(self) -> Dict:
            """Retorna métricas padrão quando não há dados"""
            return {
                "total_jobs": 0,
                "avg_execution_time": 0,
                "success_rate": 0,
                "engines": ["Spark", "OpenMP", "MPI"],
                "engine_counts": [0, 0, 0],
                "execution_times": [],
                "time_labels": [],
                "concurrent_clients": []
            }
        
        def get_recent_activity(self, limit: int = 10) -> List[Dict]:
            """Obtém atividades recentes"""
            try:
                query = {
                    "query": {"match_all": {}},
                    "sort": [{"timestamp": {"order": "desc"}}],
                    "size": limit
                }
                
                response = self.session.post(
                    f"{self.elasticsearch_url}/{self.index_name}/_search",
                    json=query
                )
                
                if response.status_code == 200:
                    result = response.json()
                    hits = result.get("hits", {}).get("hits", [])
                    return [hit["_source"] for hit in hits]
                else:
                    logger.error(f"Erro ao buscar atividades recentes: {response.text}")
                    return []
                    
            except Exception as e:
                logger.error(f"Erro ao obter atividades recentes: {e}")
                return []

    # Instância global do coletor
    metrics_collector = MetricsCollector()

---
apiVersion: v1
kind: Service
metadata:
  name: socket-server
spec:
  selector:
    app: socket-server
  ports:
  - protocol: TCP
    port: 5000
    targetPort: 5000
  type: NodePort
